# Project RoboOrchard
#
# Copyright (c) 2024-2025 Horizon Robotics. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied. See the License for the specific language governing
# permissions and limitations under the License.

import copy
from typing import Type

import cv2
import numpy as np
import pytorch_kinematics as pk
import torch
from datasets import Dataset as HFDataset
from pytorch3d.transforms import matrix_to_quaternion
from scipy.spatial.transform import Rotation

from robo_orchard_lab.dataset.robot.row_sampler import (
    MultiRowSampler,
    MultiRowSamplerConfig,
)

__all__ = [
    "ArrowDataParse",
    "EpisodeSamplerConfig",
    "AddItems",
    "AddScaleShift",
    "ConvertDataType",
    "ImageChannelFlip",
    "ItemSelection",
    "JointStateNoise",
    "SimpleStateSampling",
    "Resize",
    "ToTensor",
    "DualArmKinematics",
    "GetProjectionMat",
    "UnsqueezeBatch",
]


class IdentityTransform:
    def __call__(self, data):
        return data


class ImageChannelFlip:
    def __init__(self, output_channel=None):
        if output_channel is None:
            output_channel = [2, 1, 0]
        self.output_channel = output_channel

    def __call__(self, data):
        if isinstance(data["imgs"], (list, tuple)):
            data["imgs"] = [
                np.ascontiguousarray(x[..., self.output_channel])
                for x in data["imgs"]
            ]
        else:
            data["imgs"] = np.ascontiguousarray(
                data["imgs"][..., self.output_channel]
            )
        return data


class EpisodeSampler(MultiRowSampler):
    """A sampler for sampling complete episode data.

    For a given index, this sampler finds the complete episode that contains
    the row at that index, and returns the indices of all rows in that episode.
    This is useful for scenarios where data from all time steps within an
    episode (e.g., `joints` values) needs to be concatenated into a long
    sequence.
    """

    def __init__(self, cfg: "EpisodeSamplerConfig") -> None:
        """Initialize EpisodeSampler.

        Args:
            cfg (EpisodeSamplerConfig): Configuration for the sampler.
        """
        self.cfg = cfg

    @property
    def column_rows_keys(self) -> dict[str, list[str]]:
        """Get the target column names generated by the sampler.

        Returns a dictionary where the key is the new column name to be created
        in the final data (e.g., 'episode_joint_state'), and the value is a
        descriptive list.
        """
        ret = {}
        for column in self.cfg.target_columns:
            ret[column] = ["episode_rows"]
        return ret

    def sample_row_idx(
        self,
        index_dataset: HFDataset,
        index: int,
    ) -> dict[str, list[int | None]]:
        """Sample all row indices of the episode containing the given index.

        This method starts from `index` and scans forward and backward through
        the dataset until it finds the boundaries where `episode_index`
        changes, thus determining the range of the complete episode.

        Args:
            index_dataset (HFDataset): Dataset used for indexing, must contain
                'episode_index' column.
            index (int): Current data row index to process.

        Returns:
            dict[str, list[int | None]]: Returns a dictionary where the key is
            the target column name and the value is a list containing all row
            indices of that episode.
        """
        cur_row = index_dataset[index]
        cur_episode_idx = cur_row["episode_index"]

        # 1. Search forward to find the start boundary of the episode
        start_idx = index
        while start_idx > 0:
            prev_row = index_dataset[start_idx - 1]
            if prev_row["episode_index"] != cur_episode_idx:
                break
            start_idx -= 1

        # 2. Search backward to find the end boundary of the episode
        end_idx = index
        while end_idx < len(index_dataset) - 1:
            next_row = index_dataset[end_idx + 1]
            if next_row["episode_index"] != cur_episode_idx:
                break
            end_idx += 1

        # 3. Generate all row indices for this episode
        episode_indices: list[int | None] = list(range(start_idx, end_idx + 1))

        ret = {}
        for column in self.cfg.target_columns:
            ret[column] = episode_indices
        return ret


class EpisodeSamplerConfig(MultiRowSamplerConfig[EpisodeSampler]):
    """Configuration for the EpisodeSampler."""

    class_type: Type[EpisodeSampler] = EpisodeSampler

    # source_column: list[str]
    target_columns: list[str]


class ArrowDataParse:
    """The dataset class for manipulation tasks in RoboOrchard.

    Args:
        dataset_path (str): Path to the dataset.
        cam_names (list[str]): List of camera names to load data from.
        load_image (bool): Whether to load image data. Default is True.
        load_depth (bool): Whether to load depth data. Default is True.
        load_extrinsic (bool): Whether to load camera extrinsic data.
            Default is True.
        load_ee_state (bool): Whether to load end-effector state data.
            Default is False.
        transforms (list[dict] or dict, optional): List of transformations to
            apply to the data.
        depth_scale (float): Scale factor for depth data. Default is 1000.
        **kwargs: Additional arguments for the base RODataset class.
    """

    def __init__(
        self,
        cam_names: list[str],
        load_image=True,
        load_depth=True,
        load_extrinsic=True,
        load_ee_state=False,
        depth_scale=1000,
    ):
        """Initialize the ManipulationRODataset."""
        self.cam_names = cam_names
        self.load_image = load_image
        self.load_depth = load_depth
        self.load_extrinsic = load_extrinsic
        self.load_ee_state = load_ee_state
        self.depth_scale = depth_scale

    def get_instruction(self, data):
        """Parse instruction text from the data."""
        text = data["instruction"].json_content["description"]
        return {"text": text}

    def get_depths(self, data):
        """Parse depth images from the data."""
        depths = []
        for cam_name in self.cam_names:
            frame_id = f"{cam_name}_depth"
            depth_buffer = data[frame_id].sensor_data[0]
            decoded_depth = cv2.imdecode(
                np.frombuffer(depth_buffer, np.uint8), cv2.IMREAD_UNCHANGED
            )
            depth = decoded_depth / self.depth_scale
            depths.append(depth)
        depths = np.stack(depths)
        return {"depths": depths}

    def get_images(self, data):
        """Parse rgb images from the data."""
        images = []
        for cam_name in self.cam_names:
            frame_id = f"{cam_name}"
            img_buffer = data[frame_id].sensor_data[0]
            img_buffer = np.ndarray(
                shape=(1, len(img_buffer)), dtype=np.uint8, buffer=img_buffer
            )
            img = cv2.imdecode(img_buffer, cv2.IMREAD_ANYCOLOR)
            images.append(img)
            # del mcap_dataitem[frame_id]
        images = np.stack(images)

        return {"imgs": images}

    def get_intrinsic(self, data):
        """Parse camera intrinsic matrices from the data."""
        intrinsic = []
        for cam_name in self.cam_names:
            frame_id = f"{cam_name}"
            cam_instrinsic = np.eye(4, dtype=np.float64)
            cam_instrinsic[:3, :3] = data[frame_id].intrinsic_matrices[0]
            intrinsic.append(cam_instrinsic)
        intrinsic = np.stack(intrinsic)
        return {"intrinsic": intrinsic}

    def get_joints(self, data):
        """Parse robot joint states from the data."""
        joint_state = [item.position for item in data["joints"]]
        joint_state = np.stack(joint_state).squeeze(1).astype(np.float64)
        return {"joint_state": joint_state}

    def get_master_joints(self, data):
        """Parse master (controller) joint states from the data."""
        master_joint_state = [item.position for item in data["actions"]]
        master_joint_state = (
            np.stack(master_joint_state).squeeze(1).astype(np.float64)
        )
        return {"master_joint_state": master_joint_state}

    def get_extrinsic(self, data):
        """Parse camera extrinsic matrices from the data."""
        T_world2cam = []  # noqa: N806
        for cam_name in self.cam_names:
            frame_id = data[cam_name].frame_id
            cam_extrinsic = data[cam_name].pose

            assert cam_extrinsic.parent_frame_id == "world"
            assert (
                cam_extrinsic.child_frame_id == frame_id
                or cam_extrinsic.child_frame_id == cam_name
            )

            extrinsic = np.linalg.inv(
                data[cam_name].pose.as_Transform3D_M().get_matrix()[0].numpy()
            )
            T_world2cam.append(extrinsic)

        T_world2cam = np.stack(T_world2cam).astype(np.float64)  # noqa: N806
        return {"T_world2cam": T_world2cam}

    def __call__(self, data):
        data.update(self.get_instruction(data))
        data.update(self.get_intrinsic(data))
        data.update(self.get_joints(data))
        data.update(self.get_master_joints(data))

        if self.load_image:
            data.update(self.get_images(data))
        if self.load_depth:
            data.update(self.get_depths(data))
        if self.load_extrinsic:
            data.update(self.get_extrinsic(data))

        data["step_index"] = data["frame_index"]
        data["task_name"] = data["task"].name
        return data


class AddItems:
    def __init__(self, to_numpy=True, **kwargs):
        self.items = copy.deepcopy(kwargs)
        for k, v in self.items.items():
            if to_numpy and not isinstance(v, np.ndarray):
                self.items[k] = np.array(v)

    def __call__(self, data):
        for k, v in self.items.items():
            data[k] = copy.deepcopy(v)
        return data


class AddScaleShift:
    def __init__(self, scale_shift):
        if isinstance(scale_shift, (list, tuple)):
            scale_shift = torch.Tensor(scale_shift)
        elif isinstance(scale_shift, np.ndarray):
            scale_shift = torch.from_numpy(scale_shift)
        self.scale_shift = scale_shift

    def __call__(self, data):
        data["joint_scale_shift"] = copy.deepcopy(self.scale_shift)
        return data


class JointStateNoise:
    def __init__(self, noise_range, add_to_pred=False):
        self.range = np.array(noise_range)
        self.add_to_pred = add_to_pred

    def __call__(self, data):
        assert "hist_robot_state" not in data
        num_steps, num_joints = data["hist_joint_state"].shape
        if self.add_to_pred:
            num_steps = 1
        noise = np.random.uniform(
            self.range[..., 0],
            self.range[..., 1],
            size=[num_steps, num_joints],
        )
        noise = torch.from_numpy(noise).to(data["hist_joint_state"])
        data["hist_joint_state"] = data["hist_joint_state"] + noise
        if self.add_to_pred:
            data["pred_joint_state"] = data["pred_joint_state"] + noise
        return data


class SimpleStateSampling:
    def __init__(self, hist_steps, pred_steps):
        self.hist_steps = hist_steps
        self.pred_steps = pred_steps

    def __call__(self, data):
        if "joint_state" not in data and "hist_joint_state" in data:
            return data
        joint_state = data["joint_state"]  # N x num_joint
        step_index = data["step_index"]
        hist_steps = self.hist_steps
        pred_steps = self.pred_steps

        if "ee_state" in data:
            ee_state = data["ee_state"]  # N x [num_gripper*[xyzqxqyqzqw]]
            state = np.concatenate(
                [joint_state, ee_state],
                axis=1,
            )
        else:
            state = joint_state
        num_joint = joint_state.shape[1]

        pred_state = state[step_index + 1 : step_index + 1 + pred_steps]
        if pred_state.shape[0] != pred_steps:
            padding = np.tile(
                state[-1:], (pred_steps - pred_state.shape[0], 1)
            )
            pred_state = np.concatenate([pred_state, padding], axis=0)
        pred_joint_state = pred_state[:, :num_joint]
        if "ee_state" in data:
            pred_ee_state = pred_state[:, num_joint:]

        hist_state = state[
            max(0, step_index + 1 - hist_steps) : step_index + 1
        ]
        if hist_state.shape[0] != hist_steps:
            padding = np.tile(state[:1], (hist_steps - hist_state.shape[0], 1))
            hist_state = np.concatenate([padding, hist_state], axis=0)
        hist_joint_state = hist_state[:, :num_joint]
        if "ee_state" in data:
            hist_ee_state = hist_state[:, num_joint:]

        data.update(
            pred_joint_state=pred_joint_state,
            hist_joint_state=hist_joint_state,
        )
        data.pop("joint_state")
        if "ee_state" in data:
            data.update(
                pred_ee_state=pred_ee_state,
                hist_ee_state=hist_ee_state,
            )
            data.pop("ee_state")
        return data


class Resize:
    def __init__(self, dst_wh, dst_intrinsic=None):
        self.dst_wh = dst_wh
        if isinstance(dst_intrinsic, (list, tuple)):
            dst_intrinsic = np.array(dst_intrinsic)

        if dst_intrinsic is not None:
            _tmp = np.eye(4)
            _tmp[:3, :3] = dst_intrinsic[:3, :3]
            self.dst_intrinsic = _tmp
            u, v = np.arange(dst_wh[0]), np.arange(dst_wh[1])
            u = np.repeat(u[None], dst_wh[1], 0)
            v = np.repeat(v[:, None], dst_wh[0], 1)
            uv = np.stack([u, v, np.ones_like(u)], axis=-1)
            self.dst_pts = uv @ np.linalg.inv(self.dst_intrinsic[:3, :3]).T
        else:
            self.dst_intrinsic = None

    def __call__(self, data):
        if "imgs" in data:
            imgs = data["imgs"]
            resized_imgs = []
        else:
            imgs = None
        if "depths" in data:
            depths = data["depths"]
            resized_depths = []
        else:
            depths = None

        for i in range(data["intrinsic"].shape[0]):
            intrinsic = data["intrinsic"][i]
            inputs = []
            if imgs is not None:
                inputs.append(imgs[i])
            if depths is not None:
                inputs.append(depths[i])
            results, intrinsic = self.resize(inputs, intrinsic)
            data["intrinsic"][i] = intrinsic
            if imgs is not None:
                resized_imgs.append(results[0])
            if depths is not None:
                resized_depths.append(results[-1])
        if imgs is not None:
            data["imgs"] = np.stack(resized_imgs)
        if depths is not None:
            data["depths"] = np.stack(resized_depths)
        data["image_wh"] = np.array(data["imgs"].shape[1:3][::-1])
        return data

    def resize(self, inputs, intrinsic=None):
        if self.dst_intrinsic is not None:
            src_intrinsic = intrinsic[:3, :3]
            src_uv = self.dst_pts @ src_intrinsic.T
            src_uv = src_uv.astype(np.float32)
            for i, x in enumerate(inputs):
                inputs[i] = cv2.remap(
                    x,
                    src_uv[..., 0],
                    src_uv[..., 1],
                    cv2.INTER_LINEAR,
                )
            intrinsic = self.dst_intrinsic
        elif self.dst_wh is not None:
            origin_wh = inputs[0].shape[:2][::-1]
            trans_mat = np.eye(4)
            trans_mat[0, 0] = self.dst_wh[0] / origin_wh[0]
            trans_mat[1, 1] = self.dst_wh[1] / origin_wh[1]
            intrinsic = trans_mat @ intrinsic
            for i, x in enumerate(inputs):
                inputs[i] = cv2.resize(x, self.dst_wh)
        return inputs, intrinsic


class ToTensor:
    def __call__(self, data):
        for k, v in data.items():
            if isinstance(v, dict):
                data[k] = self.__call__(v)
            elif isinstance(v, np.ndarray):
                data[k] = torch.from_numpy(v)
            elif isinstance(v, (list, tuple)) and all(
                [isinstance(x, np.ndarray) for x in v]
            ):
                data[k] = type(v)([torch.from_numpy(x) for x in v])
        return data


class ConvertDataType:
    def __init__(self, convert_map):
        self.convert_map = convert_map

    def __call__(self, data):
        for data_name, dtype in self.convert_map.items():
            if isinstance(data[data_name], list):
                data[data_name] = torch.tensor(data[data_name])
            if isinstance(data[data_name], np.ndarray):
                data[data_name] = data[data_name].astype(dtype)
            elif isinstance(data[data_name], torch.Tensor):
                if isinstance(dtype, str):
                    dtype = getattr(torch, dtype)
                data[data_name] = data[data_name].to(dtype)
            else:
                raise TypeError(
                    f"Unsupport convert {data_name}'s "
                    f"type {type(data[data_name])} to {dtype}"
                )
        return data


class ItemSelection:
    def __init__(self, keys):
        self.keys = keys

    def __call__(self, data):
        for k in list(data.keys()):
            if k not in self.keys:
                data.pop(k)
        return data


class DualArmKinematics:
    def __init__(
        self,
        urdf,
        left_arm_link_keys=None,
        right_arm_link_keys=None,
        left_arm_joint_id=None,
        right_arm_joint_id=None,
        left_finger_keys=None,
        right_finger_keys=None,
    ):
        super().__init__()
        self.urdf = urdf
        self.chain = pk.build_chain_from_urdf(open(urdf, "rb").read())
        if left_arm_joint_id is None:
            left_arm_joint_id = [10, 11, 12, 13, 14, 15]
        if right_arm_joint_id is None:
            right_arm_joint_id = [18, 19, 20, 21, 22, 23]
        if left_arm_link_keys is None:
            left_arm_link_keys = [
                "fl_link1",
                "fl_link2",
                "fl_link3",
                "fl_link4",
                "fl_link5",
                "fl_link6",
            ]
        if right_arm_link_keys is None:
            right_arm_link_keys = [
                "fr_link1",
                "fr_link2",
                "fr_link3",
                "fr_link4",
                "fr_link5",
                "fr_link6",
            ]
        if left_finger_keys is None:
            left_finger_keys = [
                "fl_link7",
                "fl_link8",
            ]
        if right_finger_keys is None:
            right_finger_keys = [
                "fr_link7",
                "fr_link8",
            ]

        self.left_arm_joint_id = left_arm_joint_id
        self.right_arm_joint_id = right_arm_joint_id
        self.left_arm_link_keys = left_arm_link_keys
        self.right_arm_link_keys = right_arm_link_keys
        self.left_finger_keys = left_finger_keys
        self.right_finger_keys = right_finger_keys

        self.keys = (
            self.left_arm_link_keys
            + self.left_finger_keys
            + self.right_arm_link_keys
            + self.right_finger_keys
        )

    def __eq__(self, other):
        if isinstance(other, DualArmKinematics):
            return self.urdf == other.urdf
        return NotImplemented

    @property
    def joint_relative_pos(self):
        joint_idx = torch.cat(
            [
                torch.arange(len(self.left_arm_link_keys) + 1),
                torch.arange(-1, -(len(self.right_arm_link_keys) + 2), -1),
            ]
        )
        return torch.abs(joint_idx[:, None] - joint_idx)

    def __call__(self, data):
        if "pred_joint_state" in data:
            data["pred_robot_state"] = self.joint_state_to_robot_state(
                data["pred_joint_state"], data.get("embodiedment_mat")
            )
        elif "joint_state" in data:
            data["robot_state"] = self.joint_state_to_robot_state(
                data["joint_state"], data.get("embodiedment_mat")
            )

        if "hist_joint_state" in data:
            data["hist_robot_state"] = self.joint_state_to_robot_state(
                data["hist_joint_state"], data.get("embodiedment_mat")
            )
        data["joint_relative_pos"] = self.joint_relative_pos
        data["kinematics"] = self
        return data

    def joint_state_to_robot_state(
        self, joint_state, embodiedment_mat=None, return_matrix=False
    ):
        input_shape = joint_state.shape
        joint_state = joint_state.to(torch.float32)

        if (
            self.chain.device != joint_state.device
            or self.chain.dtype != joint_state.dtype
        ):
            self.chain = self.chain.to(
                device=joint_state.device,
                dtype=joint_state.dtype,
            )

        all_joint_state = torch.zeros(
            [*input_shape[:-1], len(self.chain.get_joints())]
        ).to(joint_state)
        n_left_joint = len(self.left_arm_joint_id)
        n_right_joint = len(self.right_arm_joint_id)
        all_joint_state[..., self.left_arm_joint_id] = joint_state[
            ..., :n_left_joint
        ]
        all_joint_state[..., self.right_arm_joint_id] = joint_state[
            ..., n_left_joint + 1 : n_left_joint + n_right_joint + 1
        ]
        all_joint_state = all_joint_state.flatten(end_dim=-2)
        link_poses_dict = self.chain.forward_kinematics(all_joint_state)

        link_poses = []
        for key in self.keys:
            link_poses.append(link_poses_dict[key])
        link_poses = link_poses[0].stack(*link_poses[1:])
        link_poses = link_poses.get_matrix()  # [N * xxx, 4, 4]

        if embodiedment_mat is not None:
            link_poses = embodiedment_mat @ link_poses

        if return_matrix:
            link_poses = link_poses.unflatten(0, (len(self.keys), -1))
            link_poses = link_poses.transpose(0, 1)
            link_poses = link_poses.unflatten(0, input_shape[:-1])
            return link_poses

        robot_states = torch.cat(
            [
                link_poses[..., :3, 3],
                matrix_to_quaternion(link_poses[..., :3, :3]),
            ],
            dim=-1,
        )
        robot_states = robot_states.reshape(len(self.keys), -1, 7)

        start = 0
        results = []
        for keys in [
            self.left_arm_link_keys,
            self.left_finger_keys,
            self.right_arm_link_keys,
            self.right_finger_keys,
        ]:
            end = start + len(keys)
            results.append(robot_states[start:end])
            start = end

        results[1] = results[1].mean(dim=0, keepdim=True)
        results[3] = results[3].mean(dim=0, keepdim=True)

        robot_states = torch.cat(results, dim=0)
        robot_states = robot_states.permute(1, 0, 2)
        robot_states = robot_states.reshape(*input_shape[:-1], -1, 7)
        robot_states = torch.cat(
            [joint_state[..., None], robot_states], dim=-1
        )
        return robot_states


class CalibrationToExtrinsic(DualArmKinematics):
    """Converts camera calibration parameters into extrinsic matrices.

    This class is designed to take camera calibration data (position and
    orientation) and compute the corresponding extrinsic transformations
    (i.e., the rigid transform from the camera frame to the end-effector or
    robot base frame), based on given joint indices that specify how the
    camera is mounted on the robot.

    It inherits from `DualArmKinematics`, allowing it to leverage dual-arm
    robot kinematics utilities if needed.

    Args:
        urdf (str): Path to the URDF file.
        calibration (dict, optional): A dicti containing camera calibration.
            Keys are camera names (e.g., 'left', 'right'), and values are
                dicts with:
            - 'position': list or array-like of [x, y, z].
            - 'orientation': list or array-like of [qx, qy, qz, w].
            If None, no calibration data is loaded initially.
        cam_ee_joint_indices (dict, optional): A dictionary that maps camera
            names to the index of the robot joint relative to which the
            camera's calibration is defined. Typically, this is the joint index
            of the end-effector or mounting point. If certain cameras do not
            have their cam_ee_joint_indices, their calibration represents the
            transform between the camera and the urdf base frame.
            Defaults to {'left': 5, 'right': 12}.
        cam_names (list, optional): A list of camera names specifying the
            order in which cameras are processed or expected.
        **kwargs: Additional keyword arguments passed to the parent class
            `DualArmKinematics`.
    """

    def __init__(
        self,
        urdf,
        calibration=None,
        cam_ee_joint_indices=None,
        cam_names=None,
        **kwargs,
    ):
        super().__init__(urdf, **kwargs)
        if calibration is not None:
            self.calibration = self.calibration_handler(calibration)
        else:
            self.calibration = None
        if cam_ee_joint_indices is None:
            cam_ee_joint_indices = dict(left=5, right=12)
        self.cam_ee_joint_indices = cam_ee_joint_indices
        if cam_names is None and self.calibration is not None:
            cam_names = list(self.calibration.keys())
        self.cam_names = cam_names

    def calibration_handler(self, calibration):
        calibration = copy.deepcopy(calibration)
        for k, v in calibration.items():
            if isinstance(v, dict):
                v = torch.from_numpy(self._pose_to_mat(v))
            elif isinstance(v, (list, tuple)):
                v = torch.Tensor(v)
            elif isinstance(v, np.ndarray):
                v = torch.from_numpy(v)
            calibration[k] = torch.linalg.inv(v)
        return calibration

    def __call__(self, data):
        if "calibration" in data:
            calibrations = self.calibration_handler(data["calibration"])
        else:
            calibrations = self.calibration
        if calibrations is None:
            return data
        current_joint_pose = self.joint_state_to_robot_state(
            data["hist_joint_state"][-1][None], return_matrix=True
        )[0]
        cam_names = data.get("cam_names", self.cam_names)
        t_base2cam_list = []
        for cam in cam_names:
            calibration = torch.clone(calibrations[cam])
            if cam not in self.cam_ee_joint_indices:
                t_base2cam = calibration
            else:
                idx = self.cam_ee_joint_indices[cam]
                t_ee2cam = calibration
                t_ee2base = torch.eye(4)
                t_ee2base = current_joint_pose[idx]
                t_base2cam = t_ee2cam @ torch.linalg.inv(t_ee2base).to(
                    t_ee2cam
                )
            t_base2cam_list.append(t_base2cam)
        t_base2cam = torch.stack(t_base2cam_list)
        if "T_base2world" in data:
            t_world2cam = torch.linalg.solve(
                data["T_base2world"], t_base2cam, left=False
            )
        else:
            t_world2cam = t_base2cam
        data["T_world2cam"] = t_world2cam
        return data

    def _pose_to_mat(self, pose):
        if "position" in pose:
            x, y, z = pose["position"]
        else:
            x, y, z = pose["translation"]

        if "orientation" in pose:
            qx, qy, qz, w = pose["orientation"]
        else:
            qx, qy, qz, w = pose["rotation_xyzw"]
        trans = np.array([x, y, z])
        rot = Rotation.from_quat(
            [qx, qy, qz, w], scalar_first=False
        ).as_matrix()
        ret = np.eye(4)
        ret[:3, 3] = trans
        ret[:3, :3] = rot
        return ret


class GetProjectionMat:
    def __init__(self, target_coordinate="ego"):
        assert target_coordinate in ["base", "world", "ego"]
        self.target_coordinate = target_coordinate

    def __call__(self, data):
        intrinsic = data["intrinsic"]
        if self.target_coordinate == "world":
            projection_mat = intrinsic @ data["T_world2cam"]
            embodiedment_mat = data["T_base2world"]
        elif self.target_coordinate == "base":
            projection_mat = (
                intrinsic @ data["T_world2cam"] @ data["T_base2world"]
            )
            embodiedment_mat = torch.eye(4).to(projection_mat)
        elif self.target_coordinate == "ego":
            projection_mat = (
                intrinsic
                @ data["T_world2cam"]
                @ data["T_base2world"]
                @ torch.linalg.inv(data["T_base2ego"])
            )
            embodiedment_mat = data["T_base2ego"]
        data["projection_mat"] = projection_mat
        data["embodiedment_mat"] = embodiedment_mat
        return data


class UnsqueezeBatch:
    def __call__(self, data):
        for k, v in data.items():
            if isinstance(v, (torch.Tensor, np.ndarray)):
                data[k] = v[None]
            else:
                data[k] = [v]
        return data
